{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "in this notebook we will create one dataset from datasets:\n",
    "* https://www.kaggle.com/datasets/sidarcidiacono/news-sentiment-analysis-for-stock-data-by-company\n",
    "* https://www.kaggle.com/datasets/sbhatti/financial-sentiment-analysis/data\n",
    "* https://www.kaggle.com/datasets/ankurzing/aspect-based-sentiment-analysis-for-financial-news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "path1 = kagglehub.dataset_download(\"sidarcidiacono/news-sentiment-analysis-for-stock-data-by-company\")\n",
    "path2 = kagglehub.dataset_download(\"sbhatti/financial-sentiment-analysis\")\n",
    "path3 = kagglehub.dataset_download(\"ankurzing/aspect-based-sentiment-analysis-for-financial-news\")\n",
    "\n",
    "# get csv\n",
    "# dataset 1\n",
    "csv1 = os.path.join(path1,\"djia_news copy.csv\", \"djia_news copy.csv\")\n",
    "csv2 = os.path.join(path1,\"nasdaq.csv\", \"nasdaq.csv\")\n",
    "\n",
    "# dataset 2\n",
    "csv3 = os.path.join(path2, \"data.csv\")\n",
    "\n",
    "#dataset 3 \n",
    "csv4 = os.path.join(path3, \"SEntFiN-v1.1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe\n",
    "df1 = pd.read_csv(csv1)\n",
    "df2 = pd.read_csv(csv2)\n",
    "df3 = pd.read_csv(csv3)\n",
    "df4 = pd.read_csv(csv4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's create on big df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MMM</td>\n",
       "      <td>Employer who stole nearly $3M in wages from 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MMM</td>\n",
       "      <td>Huge new Facebook data leak exposed intimate d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>MMM</td>\n",
       "      <td>A campaign has accelerated to turn a disused r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>MMM</td>\n",
       "      <td>Google launches global human trafficking helpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>MMM</td>\n",
       "      <td>Over 3m Saudi Women Don’t Have ID Cards; Saudi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>0</td>\n",
       "      <td>WMT</td>\n",
       "      <td>Walmart dumps e-cigarettes: Largest store in U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>0</td>\n",
       "      <td>WMT</td>\n",
       "      <td>Walmart makes a $16 billion bet on India's boo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378</th>\n",
       "      <td>0</td>\n",
       "      <td>WMT</td>\n",
       "      <td>Walmart raises minimum age to buy tobacco to 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>0</td>\n",
       "      <td>WMT</td>\n",
       "      <td>Walmart Took Over Chile In Only Three Years An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>2</td>\n",
       "      <td>WMT</td>\n",
       "      <td>Carla Cheney: Walmart Fired Me For Reporting D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2381 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label Ticker                                           Headline\n",
       "0         0    MMM  Employer who stole nearly $3M in wages from 15...\n",
       "1         1    MMM  Huge new Facebook data leak exposed intimate d...\n",
       "2         0    MMM  A campaign has accelerated to turn a disused r...\n",
       "3         1    MMM  Google launches global human trafficking helpl...\n",
       "4         1    MMM  Over 3m Saudi Women Don’t Have ID Cards; Saudi...\n",
       "...     ...    ...                                                ...\n",
       "2376      0    WMT  Walmart dumps e-cigarettes: Largest store in U...\n",
       "2377      0    WMT  Walmart makes a $16 billion bet on India's boo...\n",
       "2378      0    WMT  Walmart raises minimum age to buy tobacco to 2...\n",
       "2379      0    WMT  Walmart Took Over Chile In Only Three Years An...\n",
       "2380      2    WMT  Carla Cheney: Walmart Fired Me For Reporting D...\n",
       "\n",
       "[2381 rows x 3 columns]"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Employer who stole nearly $3M in wages from 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Huge new Facebook data leak exposed intimate d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A campaign has accelerated to turn a disused r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Google launches global human trafficking helpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Over 3m Saudi Women Don’t Have ID Cards; Saudi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>0</td>\n",
       "      <td>Walmart dumps e-cigarettes: Largest store in U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>0</td>\n",
       "      <td>Walmart makes a $16 billion bet on India's boo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378</th>\n",
       "      <td>0</td>\n",
       "      <td>Walmart raises minimum age to buy tobacco to 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>0</td>\n",
       "      <td>Walmart Took Over Chile In Only Three Years An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>2</td>\n",
       "      <td>Carla Cheney: Walmart Fired Me For Reporting D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2381 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentiment                                           Headline\n",
       "0             0  Employer who stole nearly $3M in wages from 15...\n",
       "1             1  Huge new Facebook data leak exposed intimate d...\n",
       "2             0  A campaign has accelerated to turn a disused r...\n",
       "3             1  Google launches global human trafficking helpl...\n",
       "4             1  Over 3m Saudi Women Don’t Have ID Cards; Saudi...\n",
       "...         ...                                                ...\n",
       "2376          0  Walmart dumps e-cigarettes: Largest store in U...\n",
       "2377          0  Walmart makes a $16 billion bet on India's boo...\n",
       "2378          0  Walmart raises minimum age to buy tobacco to 2...\n",
       "2379          0  Walmart Took Over Chile In Only Three Years An...\n",
       "2380          2  Carla Cheney: Walmart Fired Me For Reporting D...\n",
       "\n",
       "[2381 rows x 2 columns]"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df1.drop(columns=['Ticker'])\n",
    "df1.columns = ['Sentiment', \"Headline\"]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@TotesTravel : Airline shares tumble as New Yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@TotesTravel : American United call off Hong K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@TotesTravel : U.S. airline stocks hit highest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>@TotesTravel : American Airlines reaches deal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>@TotesTravel : US airlines Treasury Department...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13176</th>\n",
       "      <td>1</td>\n",
       "      <td>Bitcoin Tops $1000 Again as Zynga Accepts Virt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13177</th>\n",
       "      <td>1</td>\n",
       "      <td>Zynga Accepts Bitcoin For Microtransactions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13178</th>\n",
       "      <td>1</td>\n",
       "      <td>Zumiez (ZUMZ) unusual put activity into earnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13179</th>\n",
       "      <td>1</td>\n",
       "      <td>Zumiez Is Going Bankrupt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13180</th>\n",
       "      <td>1</td>\n",
       "      <td>Zumiez Is Going Bankrupt!! Omfg!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13181 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentiment                                           Headline\n",
       "0              0  @TotesTravel : Airline shares tumble as New Yo...\n",
       "1              1  @TotesTravel : American United call off Hong K...\n",
       "2              0  @TotesTravel : U.S. airline stocks hit highest...\n",
       "3              1  @TotesTravel : American Airlines reaches deal ...\n",
       "4              1  @TotesTravel : US airlines Treasury Department...\n",
       "...          ...                                                ...\n",
       "13176          1  Bitcoin Tops $1000 Again as Zynga Accepts Virt...\n",
       "13177          1        Zynga Accepts Bitcoin For Microtransactions\n",
       "13178          1  Zumiez (ZUMZ) unusual put activity into earnin...\n",
       "13179          1                           Zumiez Is Going Bankrupt\n",
       "13180          1                   Zumiez Is Going Bankrupt!! Omfg!\n",
       "\n",
       "[13181 rows x 2 columns]"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2.drop(columns=['Ticker'])\n",
    "df2.columns = ['Sentiment', \"Headline\"]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5837</th>\n",
       "      <td>RISING costs have forced packaging producer Hu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5838</th>\n",
       "      <td>Nordic Walking was first used as a summer trai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5839</th>\n",
       "      <td>According shipping company Viking Line , the E...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5840</th>\n",
       "      <td>In the building and home improvement trade , s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5841</th>\n",
       "      <td>HELSINKI AFX - KCI Konecranes said it has won ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5842 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Headline  Sentiment\n",
       "0     The GeoSolutions technology will leverage Bene...          2\n",
       "1     $ESI on lows, down $1.50 to $2.50 BK a real po...          0\n",
       "2     For the last quarter of 2010 , Componenta 's n...          2\n",
       "3     According to the Finnish-Russian Chamber of Co...          1\n",
       "4     The Swedish buyout firm has sold its remaining...          1\n",
       "...                                                 ...        ...\n",
       "5837  RISING costs have forced packaging producer Hu...          0\n",
       "5838  Nordic Walking was first used as a summer trai...          1\n",
       "5839  According shipping company Viking Line , the E...          1\n",
       "5840  In the building and home improvement trade , s...          1\n",
       "5841  HELSINKI AFX - KCI Konecranes said it has won ...          2\n",
       "\n",
       "[5842 rows x 2 columns]"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_dict = {'positive': 2,\n",
    "            'neutral': 1,\n",
    "            'negative': 0}\n",
    "\n",
    "df3['Sentiment'] = df3['Sentiment'].map(map_dict)\n",
    "df3.columns = ['Headline', \"Sentiment\"]\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SpiceJet to issue 6.4 crore warrants to promoters</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MMTC Q2 net loss at Rs 10.4 crore</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mid-cap funds can deliver more, stay put: Experts</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mid caps now turn into market darlings</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Market seeing patience, if not conviction: Pra...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10748</th>\n",
       "      <td>Negative on Chambal, Advanta: Mitesh Thacker</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10749</th>\n",
       "      <td>Small, Mid-cap stocks may emerge outperformers</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10750</th>\n",
       "      <td>Rupee slips against US dollar</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10751</th>\n",
       "      <td>Rupee weak against US dollar</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10752</th>\n",
       "      <td>Australia shares flat; energy drags</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10753 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Headline  Sentiment\n",
       "0      SpiceJet to issue 6.4 crore warrants to promoters        1.0\n",
       "1                      MMTC Q2 net loss at Rs 10.4 crore        1.0\n",
       "2      Mid-cap funds can deliver more, stay put: Experts        2.0\n",
       "3                 Mid caps now turn into market darlings        2.0\n",
       "4      Market seeing patience, if not conviction: Pra...        1.0\n",
       "...                                                  ...        ...\n",
       "10748       Negative on Chambal, Advanta: Mitesh Thacker        0.0\n",
       "10749     Small, Mid-cap stocks may emerge outperformers        2.0\n",
       "10750                      Rupee slips against US dollar        0.5\n",
       "10751                       Rupee weak against US dollar        0.5\n",
       "10752                Australia shares flat; energy drags        1.0\n",
       "\n",
       "[10753 rows x 2 columns]"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "df4 = df4.drop(columns=['S No.', 'Words'])\n",
    "df4.columns = ['Headline', 'Sentiment']\n",
    "\n",
    "def fun_map(j):\n",
    "    data = json.loads(j)\n",
    "    sentiment = [map_dict[value] for key, value in data.items()]    \n",
    "    return sum(sentiment) / len(sentiment)\n",
    "\n",
    "df4['Sentiment'] = df4['Sentiment'].map(fun_map)\n",
    "\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>Employer who stole nearly $3M in wages from 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>Huge new Facebook data leak exposed intimate d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>A campaign has accelerated to turn a disused r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.50</td>\n",
       "      <td>Google launches global human trafficking helpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.50</td>\n",
       "      <td>Over 3m Saudi Women Don’t Have ID Cards; Saudi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32152</th>\n",
       "      <td>0.00</td>\n",
       "      <td>Negative on Chambal, Advanta: Mitesh Thacker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32153</th>\n",
       "      <td>1.00</td>\n",
       "      <td>Small, Mid-cap stocks may emerge outperformers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32154</th>\n",
       "      <td>0.25</td>\n",
       "      <td>Rupee slips against US dollar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32155</th>\n",
       "      <td>0.25</td>\n",
       "      <td>Rupee weak against US dollar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32156</th>\n",
       "      <td>0.50</td>\n",
       "      <td>Australia shares flat; energy drags</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32157 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentiment                                           Headline\n",
       "0           0.00  Employer who stole nearly $3M in wages from 15...\n",
       "1           0.50  Huge new Facebook data leak exposed intimate d...\n",
       "2           0.00  A campaign has accelerated to turn a disused r...\n",
       "3           0.50  Google launches global human trafficking helpl...\n",
       "4           0.50  Over 3m Saudi Women Don’t Have ID Cards; Saudi...\n",
       "...          ...                                                ...\n",
       "32152       0.00       Negative on Chambal, Advanta: Mitesh Thacker\n",
       "32153       1.00     Small, Mid-cap stocks may emerge outperformers\n",
       "32154       0.25                      Rupee slips against US dollar\n",
       "32155       0.25                       Rupee weak against US dollar\n",
       "32156       0.50                Australia shares flat; energy drags\n",
       "\n",
       "[32157 rows x 2 columns]"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df1, df2, df3, df4], axis=0, ignore_index=True)\n",
    "df['Sentiment'] = df['Sentiment'].map(lambda x: x / 2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have to clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A campaign has accelerated to turn a disused railway line in Yorkshire into England’s longest cycle tunnel – instead of using £3m of public money to close it for ever. Campaigners say they could enhance West Yorkshire’s health and economy by converting an old railway line.'"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2]['Headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Employer who stole nearly 3M in wages from 157...\n",
       "1        Huge new Facebook data leak exposed intimate d...\n",
       "2        A campaign has accelerated to turn a disused r...\n",
       "3        Google launches global human trafficking helpl...\n",
       "4        Over 3m Saudi Women Dont Have ID Cards Saudi G...\n",
       "                               ...                        \n",
       "32152           Negative on Chambal Advanta Mitesh Thacker\n",
       "32153         Small Midcap stocks may emerge outperformers\n",
       "32154                        Rupee slips against US dollar\n",
       "32155                         Rupee weak against US dollar\n",
       "32156                   Australia shares flat energy drags\n",
       "Name: Headline, Length: 32157, dtype: object"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "def punctuation_removal(text: str) -> str: \n",
    "    return \"\".join([u for u in text if u == ' ' or u == \"'\" or ord('0') <= ord(u) <= ord('9') or ord('a') <= ord(u) <= ord('z') or ord('A') <= ord(u) <= ord('Z')])\n",
    "\n",
    "df['Headline'] = df['Headline'].map(punctuation_removal)\n",
    "df['Headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google launches global human trafficking helpline amp data network  Commits 3M 2M to build an international helpline network fueled by data Human trafficking enslaves 21M people with 25M forced into labor Most are ages 1824 amp 43 are forced into the sex trade'"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Headline'].iloc[3] # we have to remove &amp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google launches global human trafficking helpline  data network  commits 3m 2m to build an international helpline network fueled by data human trafficking enslaves 21m people with 25m forced into labor most are ages 1824  43 are forced into the sex trade'"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Headline'] = df['Headline'].map(lambda s: s.replace('amp', '').lower())\n",
    "df['Headline'].iloc[3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>employer who stole nearly 3m in wages from 157...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>huge new facebook data leak exposed intimate d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>a caign has accelerated to turn a disused rail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.50</td>\n",
       "      <td>google launches global human trafficking helpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.50</td>\n",
       "      <td>over 3m saudi women dont have id cards saudi g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32152</th>\n",
       "      <td>0.00</td>\n",
       "      <td>negative on chambal advanta mitesh thacker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32153</th>\n",
       "      <td>1.00</td>\n",
       "      <td>small midcap stocks may emerge outperformers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32154</th>\n",
       "      <td>0.25</td>\n",
       "      <td>rupee slips against us dollar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32155</th>\n",
       "      <td>0.25</td>\n",
       "      <td>rupee weak against us dollar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32156</th>\n",
       "      <td>0.50</td>\n",
       "      <td>australia shares flat energy drags</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32157 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentiment                                           Headline\n",
       "0           0.00  employer who stole nearly 3m in wages from 157...\n",
       "1           0.50  huge new facebook data leak exposed intimate d...\n",
       "2           0.00  a caign has accelerated to turn a disused rail...\n",
       "3           0.50  google launches global human trafficking helpl...\n",
       "4           0.50  over 3m saudi women dont have id cards saudi g...\n",
       "...          ...                                                ...\n",
       "32152       0.00         negative on chambal advanta mitesh thacker\n",
       "32153       1.00       small midcap stocks may emerge outperformers\n",
       "32154       0.25                      rupee slips against us dollar\n",
       "32155       0.25                       rupee weak against us dollar\n",
       "32156       0.50                 australia shares flat energy drags\n",
       "\n",
       "[32157 rows x 2 columns]"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have to analyse dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 16003),\n",
       " ('to', 13799),\n",
       " ('of', 11258),\n",
       " ('in', 11254),\n",
       " ('and', 7667),\n",
       " ('a', 7373),\n",
       " ('on', 5918),\n",
       " ('for', 5616),\n",
       " ('as', 3092),\n",
       " ('is', 3025),\n",
       " ('from', 2713),\n",
       " ('with', 2626),\n",
       " ('by', 2612),\n",
       " ('at', 2453),\n",
       " ('that', 1964),\n",
       " ('has', 1930),\n",
       " ('it', 1776),\n",
       " ('its', 1748),\n",
       " ('us', 1737),\n",
       " ('will', 1723),\n",
       " ('be', 1619),\n",
       " ('rs', 1587),\n",
       " ('up', 1547),\n",
       " ('after', 1514),\n",
       " ('net', 1452),\n",
       " ('new', 1433),\n",
       " ('over', 1388),\n",
       " ('company', 1338),\n",
       " ('eur', 1284),\n",
       " ('an', 1200),\n",
       " ('are', 1180),\n",
       " ('profit', 1165),\n",
       " ('have', 1128),\n",
       " ('first', 1111),\n",
       " ('was', 1077),\n",
       " ('bank', 1052),\n",
       " ('market', 1028),\n",
       " ('stocks', 1008),\n",
       " ('million', 977),\n",
       " ('said', 973),\n",
       " ('inc', 956),\n",
       " (\"'s\", 948),\n",
       " ('crore', 941),\n",
       " ('shares', 898),\n",
       " ('says', 856),\n",
       " ('down', 837),\n",
       " ('group', 827),\n",
       " ('india', 811),\n",
       " ('more', 804),\n",
       " ('china', 798),\n",
       " ('not', 764),\n",
       " ('mn', 763),\n",
       " ('sales', 708),\n",
       " ('global', 706),\n",
       " ('their', 706),\n",
       " ('this', 703),\n",
       " ('may', 697),\n",
       " ('oil', 696),\n",
       " ('news', 684),\n",
       " ('year', 675),\n",
       " ('than', 659),\n",
       " ('per', 631),\n",
       " ('about', 625),\n",
       " ('power', 617),\n",
       " ('out', 615),\n",
       " ('stock', 605),\n",
       " ('corporation', 601),\n",
       " ('world', 594),\n",
       " ('been', 587),\n",
       " ('government', 579),\n",
       " ('into', 561),\n",
       " ('financial', 551),\n",
       " ('against', 547),\n",
       " ('energy', 540),\n",
       " ('finnish', 530),\n",
       " ('billion', 520),\n",
       " ('companies', 518),\n",
       " ('services', 492),\n",
       " ('now', 491),\n",
       " ('quarter', 489),\n",
       " ('one', 488),\n",
       " ('apple', 487),\n",
       " ('capital', 479),\n",
       " ('top', 474),\n",
       " ('which', 468),\n",
       " ('chinese', 466),\n",
       " ('all', 463),\n",
       " ('high', 458),\n",
       " ('share', 457),\n",
       " ('nifty', 455),\n",
       " ('trump', 454),\n",
       " ('business', 447),\n",
       " ('can', 446),\n",
       " ('they', 444),\n",
       " ('loss', 444),\n",
       " ('report', 443),\n",
       " ('cent', 442),\n",
       " ('who', 441),\n",
       " ('most', 438),\n",
       " ('time', 437),\n",
       " ('people', 436),\n",
       " ('his', 429),\n",
       " ('buy', 428),\n",
       " ('uk', 421),\n",
       " ('but', 418),\n",
       " ('years', 415),\n",
       " ('two', 414),\n",
       " ('operating', 413),\n",
       " ('markets', 412),\n",
       " ('international', 411),\n",
       " ('no', 409),\n",
       " ('could', 408),\n",
       " ('demand', 408),\n",
       " ('data', 406),\n",
       " ('gold', 406),\n",
       " ('deal', 388),\n",
       " ('price', 386),\n",
       " ('or', 385),\n",
       " ('systems', 382),\n",
       " ('period', 381),\n",
       " ('he', 379),\n",
       " ('ceo', 379),\n",
       " ('2009', 374),\n",
       " ('growth', 368),\n",
       " ('m', 368),\n",
       " ('other', 367),\n",
       " ('off', 366),\n",
       " ('2008', 362),\n",
       " ('were', 358),\n",
       " ('would', 357),\n",
       " ('sebi', 357),\n",
       " ('day', 355),\n",
       " ('investors', 355),\n",
       " ('mln', 352),\n",
       " ('president', 351),\n",
       " ('russia', 343),\n",
       " ('1', 342),\n",
       " ('also', 339),\n",
       " ('prices', 339),\n",
       " ('facebook', 338),\n",
       " ('google', 338),\n",
       " ('10', 336),\n",
       " ('some', 334),\n",
       " ('court', 332),\n",
       " ('5', 332),\n",
       " ('q1', 330),\n",
       " ('discovery', 327),\n",
       " ('2', 326),\n",
       " ('finland', 326),\n",
       " ('cr', 324),\n",
       " ('fall', 323),\n",
       " ('euro', 320),\n",
       " ('today', 319),\n",
       " ('q2', 316),\n",
       " ('australian', 315),\n",
       " ('media', 313),\n",
       " ('popular', 313),\n",
       " ('had', 312),\n",
       " ('futures', 311),\n",
       " ('sell', 307),\n",
       " ('good', 305),\n",
       " ('just', 304),\n",
       " ('ltd', 303),\n",
       " ('trade', 301),\n",
       " ('plans', 301),\n",
       " ('russian', 300),\n",
       " ('american', 299),\n",
       " ('we', 299),\n",
       " ('korea', 298),\n",
       " ('2007', 298),\n",
       " ('3', 297),\n",
       " ('results', 296),\n",
       " ('police', 294),\n",
       " ('make', 294),\n",
       " ('hit', 293),\n",
       " ('industry', 291),\n",
       " ('under', 290),\n",
       " ('sensex', 290),\n",
       " ('you', 288),\n",
       " ('stake', 287),\n",
       " ('last', 287),\n",
       " ('end', 287),\n",
       " ('ipo', 287),\n",
       " ('back', 285),\n",
       " ('20', 285),\n",
       " ('technology', 284),\n",
       " ('how', 283),\n",
       " ('microsoft', 282),\n",
       " ('dollar', 280),\n",
       " ('alphabet', 279),\n",
       " ('q3', 277),\n",
       " ('united', 277),\n",
       " ('earnings', 277),\n",
       " ('next', 274),\n",
       " ('farmers', 273),\n",
       " ('s', 273),\n",
       " ('2010', 271),\n",
       " ('since', 270),\n",
       " ('view', 269),\n",
       " ('vaccine', 266),\n",
       " ('say', 265),\n",
       " ('get', 265),\n",
       " ('security', 265),\n",
       " ('state', 263),\n",
       " ('q4', 263),\n",
       " ('before', 260),\n",
       " ('according', 258),\n",
       " ('investment', 258),\n",
       " ('during', 257),\n",
       " ('like', 256),\n",
       " ('de', 255),\n",
       " ('former', 255),\n",
       " ('likely', 254),\n",
       " ('low', 254),\n",
       " ('including', 253),\n",
       " ('hits', 253),\n",
       " ('biggest', 252),\n",
       " ('order', 252),\n",
       " ('european', 252),\n",
       " ('ban', 252),\n",
       " ('found', 251),\n",
       " ('ahead', 251),\n",
       " ('oyj', 250),\n",
       " ('change', 249),\n",
       " ('support', 249),\n",
       " ('citizens', 249),\n",
       " ('south', 248),\n",
       " ('home', 248),\n",
       " ('country', 248),\n",
       " ('week', 247),\n",
       " ('should', 243),\n",
       " ('through', 243),\n",
       " ('take', 243),\n",
       " ('climate', 243),\n",
       " ('indian', 242),\n",
       " ('use', 240),\n",
       " ('north', 239),\n",
       " ('software', 239),\n",
       " ('sale', 239),\n",
       " ('if', 238),\n",
       " ('system', 238),\n",
       " ('4', 238),\n",
       " ('rise', 238),\n",
       " ('reports', 237),\n",
       " ('target', 236),\n",
       " ('ericsson', 236),\n",
       " ('lower', 236),\n",
       " ('online', 233),\n",
       " ('record', 232),\n",
       " ('major', 232),\n",
       " ('months', 232),\n",
       " ('set', 231),\n",
       " ('tech', 229),\n",
       " ('what', 228),\n",
       " ('solar', 228),\n",
       " ('trading', 227),\n",
       " ('sector', 227),\n",
       " ('pc', 227),\n",
       " ('percent', 226),\n",
       " ('between', 225),\n",
       " ('our', 225),\n",
       " ('board', 225),\n",
       " ('near', 225),\n",
       " ('national', 223),\n",
       " ('second', 223),\n",
       " ('gaming', 223),\n",
       " ('rights', 221),\n",
       " ('banks', 221),\n",
       " ('part', 220),\n",
       " ('gas', 220),\n",
       " ('ashwani', 220),\n",
       " ('rupee', 220),\n",
       " ('compared', 219),\n",
       " ('internet', 219),\n",
       " ('levels', 219),\n",
       " ('gujral', 219),\n",
       " ('made', 218),\n",
       " ('products', 217),\n",
       " ('being', 217),\n",
       " ('smith', 216),\n",
       " ('help', 215),\n",
       " ('i', 215),\n",
       " ('countries', 214),\n",
       " ('key', 214),\n",
       " ('expected', 214),\n",
       " ('because', 213),\n",
       " ('so', 212),\n",
       " ('pharma', 212),\n",
       " ('post', 212),\n",
       " ('space', 211),\n",
       " ('air', 211),\n",
       " ('city', 211),\n",
       " ('15', 211),\n",
       " ('making', 210),\n",
       " ('sp', 210),\n",
       " ('operations', 209),\n",
       " ('charles', 209),\n",
       " ('service', 209),\n",
       " ('your', 208),\n",
       " ('falls', 208),\n",
       " ('german', 208),\n",
       " ('research', 208),\n",
       " ('public', 207),\n",
       " ('states', 207),\n",
       " ('6', 207),\n",
       " ('100', 206),\n",
       " ('8', 206),\n",
       " ('federal', 206),\n",
       " ('move', 205),\n",
       " ('largest', 205),\n",
       " ('only', 205),\n",
       " ('nearly', 202),\n",
       " ('gains', 202),\n",
       " ('well', 201),\n",
       " ('chief', 201),\n",
       " ('noodles', 201),\n",
       " ('focus', 200),\n",
       " ('street', 200),\n",
       " ('auto', 199),\n",
       " ('dave', 199),\n",
       " ('three', 198),\n",
       " ('c', 198),\n",
       " ('around', 197),\n",
       " ('mobile', 197),\n",
       " ('plan', 196),\n",
       " ('stop', 195),\n",
       " ('tesla', 195),\n",
       " ('war', 194),\n",
       " ('rally', 193),\n",
       " ('long', 192),\n",
       " ('contract', 192),\n",
       " ('plant', 192),\n",
       " ('rises', 192),\n",
       " ('money', 191),\n",
       " ('firm', 191),\n",
       " ('them', 191),\n",
       " ('workers', 190),\n",
       " ('march', 190),\n",
       " (\"''\", 189),\n",
       " ('show', 188),\n",
       " ('fund', 188),\n",
       " ('2006', 188),\n",
       " ('higher', 187),\n",
       " ('go', 187),\n",
       " ('value', 187),\n",
       " ('seen', 187),\n",
       " ('cut', 186),\n",
       " ('while', 186),\n",
       " ('when', 186),\n",
       " ('pay', 185),\n",
       " ('amazon', 185),\n",
       " ('weak', 185),\n",
       " ('canada', 184),\n",
       " ('hong', 184),\n",
       " ('healthcare', 184),\n",
       " ('see', 183),\n",
       " ('management', 183),\n",
       " ('total', 183),\n",
       " ('coronavirus', 182),\n",
       " ('big', 182),\n",
       " ('increased', 182),\n",
       " ('points', 181),\n",
       " ('nokia', 181),\n",
       " ('health', 180),\n",
       " ('man', 179),\n",
       " ('used', 179),\n",
       " ('video', 179),\n",
       " ('free', 179),\n",
       " ('30', 179),\n",
       " ('12', 178),\n",
       " ('offer', 178),\n",
       " ('drug', 178),\n",
       " ('bond', 178),\n",
       " ('ss', 178),\n",
       " ('call', 177),\n",
       " ('issue', 177),\n",
       " ('exchange', 177),\n",
       " ('water', 177),\n",
       " ('mitesh', 177),\n",
       " ('thacker', 177),\n",
       " ('five', 176),\n",
       " ('strong', 175),\n",
       " ('due', 175),\n",
       " ('googul', 175),\n",
       " ('bonds', 175),\n",
       " ('japan', 174),\n",
       " ('foreign', 174),\n",
       " ('kong', 174),\n",
       " ('customers', 173),\n",
       " ('announced', 173),\n",
       " ('credit', 172),\n",
       " ('unit', 172),\n",
       " ('bill', 171),\n",
       " ('production', 171),\n",
       " ('worth', 171),\n",
       " ('below', 171),\n",
       " ('minister', 171),\n",
       " ('helsinki', 171),\n",
       " ('do', 170),\n",
       " ('australia', 169),\n",
       " ('open', 169),\n",
       " ('eu', 168),\n",
       " ('europe', 167),\n",
       " ('technologies', 167),\n",
       " ('nasdaq', 167),\n",
       " ('7', 167),\n",
       " ('raise', 165),\n",
       " ('among', 165),\n",
       " ('third', 165),\n",
       " ('friday', 165),\n",
       " ('head', 164),\n",
       " ('life', 164),\n",
       " ('human', 163),\n",
       " ('tax', 163),\n",
       " (\"world's\", 163),\n",
       " ('2014', 163),\n",
       " ('beyond', 163),\n",
       " ('abc', 163),\n",
       " ('supply', 162),\n",
       " ('50', 162),\n",
       " ('best', 162),\n",
       " ('analysis', 162),\n",
       " ('broadcasting', 162),\n",
       " ('asia', 161),\n",
       " ('still', 161),\n",
       " ('any', 161),\n",
       " ('ibm', 161),\n",
       " ('live', 160),\n",
       " ('increase', 160),\n",
       " ('house', 159),\n",
       " ('there', 159),\n",
       " ('covid19', 159),\n",
       " ('way', 159),\n",
       " ('food', 158),\n",
       " ('british', 158),\n",
       " ('why', 158),\n",
       " ('banking', 158),\n",
       " ('solutions', 158),\n",
       " ('corresponding', 158),\n",
       " ('canadian', 157),\n",
       " ('military', 157),\n",
       " ('steel', 157),\n",
       " ('joint', 157),\n",
       " ('where', 156),\n",
       " ('buying', 156),\n",
       " ('industries', 156),\n",
       " ('network', 155),\n",
       " ('case', 155),\n",
       " ('crisis', 155),\n",
       " ('her', 155),\n",
       " ('israel', 154),\n",
       " ('corp', 154),\n",
       " ('development', 153),\n",
       " ('sony', 153),\n",
       " ('using', 152),\n",
       " ('attack', 152),\n",
       " ('even', 152),\n",
       " ('securities', 152),\n",
       " ('nuclear', 151),\n",
       " ('employees', 151),\n",
       " ('times', 151),\n",
       " ('four', 151),\n",
       " ('germany', 151),\n",
       " ('agreement', 151),\n",
       " ('building', 150),\n",
       " ('sanofi', 150),\n",
       " ('firms', 149),\n",
       " ('information', 149),\n",
       " ('half', 149),\n",
       " ('coal', 149),\n",
       " ('rallies', 149),\n",
       " ('project', 148),\n",
       " ('review', 148),\n",
       " ('slips', 148),\n",
       " ('number', 147),\n",
       " ('phone', 147),\n",
       " ('despite', 146),\n",
       " ('director', 146),\n",
       " ('goldman', 145),\n",
       " ('policy', 145),\n",
       " ('revenue', 145),\n",
       " ('lead', 145),\n",
       " ('25', 145),\n",
       " ('surges', 145),\n",
       " ('cues', 145),\n",
       " ('line', 144),\n",
       " ('giant', 144),\n",
       " ('france', 144),\n",
       " ('officials', 144),\n",
       " ('intel', 144),\n",
       " ('current', 144),\n",
       " ('dow', 144),\n",
       " ('communications', 144),\n",
       " ('11', 143),\n",
       " ('launch', 143),\n",
       " ('flat', 143),\n",
       " ('500', 142),\n",
       " ('donald', 142),\n",
       " ('social', 142),\n",
       " ('calls', 142),\n",
       " ('premier', 142),\n",
       " ('bse', 142),\n",
       " ('face', 141),\n",
       " ('visa', 141),\n",
       " ('same', 141),\n",
       " ('9', 141),\n",
       " ('monday', 141),\n",
       " ('johnson', 140),\n",
       " ('days', 140),\n",
       " ('netflix', 140),\n",
       " ('fire', 140),\n",
       " ('earlier', 140),\n",
       " ('close', 139),\n",
       " ('law', 139),\n",
       " ('continue', 139),\n",
       " ('french', 138),\n",
       " ('maker', 138),\n",
       " ('gets', 137),\n",
       " ('cisco', 137),\n",
       " ('look', 137),\n",
       " ('rate', 135),\n",
       " ('sachs', 135),\n",
       " ('posts', 135),\n",
       " ('walmart', 135),\n",
       " ('gain', 135),\n",
       " ('ever', 134),\n",
       " ('saudi', 134),\n",
       " ('appoints', 134),\n",
       " ('amid', 134),\n",
       " ('action', 134),\n",
       " ('result', 134),\n",
       " ('finance', 134),\n",
       " ('give', 134),\n",
       " ('such', 133),\n",
       " ('wednesday', 133),\n",
       " (\"china's\", 133),\n",
       " ('boeing', 133),\n",
       " ('small', 132),\n",
       " ('via', 132),\n",
       " ('israeli', 132),\n",
       " ('announces', 132),\n",
       " ('18', 132),\n",
       " ('construction', 132),\n",
       " ('chevron', 132),\n",
       " ('decreased', 132),\n",
       " ('west', 131),\n",
       " ('chairman', 131),\n",
       " ('shows', 131),\n",
       " ('own', 131),\n",
       " ('going', 131),\n",
       " ('funds', 131),\n",
       " ('latest', 130),\n",
       " ('2015', 130),\n",
       " ('deepak', 130),\n",
       " ('accused', 129),\n",
       " ('scientists', 129),\n",
       " ('term', 129),\n",
       " ('cash', 128),\n",
       " ('my', 128),\n",
       " ('cocacola', 128),\n",
       " ('infosys', 128),\n",
       " ('users', 127),\n",
       " ('test', 127),\n",
       " ('jobs', 127),\n",
       " ('access', 127),\n",
       " ('iran', 127),\n",
       " ('central', 127),\n",
       " ('product', 127),\n",
       " ('forecast', 127),\n",
       " ('wall', 127),\n",
       " ('level', 127),\n",
       " ('positive', 126),\n",
       " ('fossil', 126),\n",
       " ('tripadvisor', 125),\n",
       " ('executive', 125),\n",
       " ('pm', 125),\n",
       " ('rules', 125),\n",
       " ('tv', 125),\n",
       " ('become', 125),\n",
       " ('watch', 125),\n",
       " ('john', 125),\n",
       " ('selling', 124),\n",
       " ('rating', 124),\n",
       " ('london', 124),\n",
       " ('very', 124),\n",
       " ('remain', 124),\n",
       " ('tata', 124),\n",
       " ('institutions', 124),\n",
       " ('hel', 124),\n",
       " ('un', 123),\n",
       " (\"it's\", 123),\n",
       " ('killed', 122),\n",
       " ('daily', 122),\n",
       " ('tuesday', 122),\n",
       " ('index', 122),\n",
       " ('study', 122),\n",
       " ('jumps', 122),\n",
       " ('rose', 122),\n",
       " ('14', 122),\n",
       " ('above', 121),\n",
       " ('drop', 121),\n",
       " ('right', 121),\n",
       " ('work', 121),\n",
       " ('further', 121),\n",
       " ('need', 121),\n",
       " ('debt', 120),\n",
       " ('16', 120),\n",
       " ('many', 120),\n",
       " ('nike', 120),\n",
       " ('drops', 120),\n",
       " ('prince', 120),\n",
       " ('september', 120),\n",
       " ('2005', 120),\n",
       " ('fuel', 119),\n",
       " ('moves', 119),\n",
       " ('17', 119),\n",
       " ('prime', 119),\n",
       " ('short', 119),\n",
       " ('airbnb', 119),\n",
       " ('rbi', 119),\n",
       " ('across', 118),\n",
       " ('future', 118),\n",
       " ('agency', 118),\n",
       " ('infrastructure', 118),\n",
       " ('fresh', 118),\n",
       " (\"'\", 118),\n",
       " ('real', 118),\n",
       " ('marriott', 118),\n",
       " ('launches', 117),\n",
       " ('following', 117),\n",
       " ('40', 117),\n",
       " ('almost', 117),\n",
       " ('app', 117),\n",
       " ('thursday', 117),\n",
       " ('reported', 117),\n",
       " ('land', 117),\n",
       " ('start', 117),\n",
       " ('early', 117),\n",
       " ('takes', 117),\n",
       " ('projects', 117),\n",
       " ('plc', 117),\n",
       " ('pct', 117),\n",
       " ('strike', 116),\n",
       " ('2020', 116),\n",
       " ('disney', 116),\n",
       " ('pressure', 115),\n",
       " ('co', 115),\n",
       " ('washington', 115),\n",
       " ('royal', 115),\n",
       " ('office', 115),\n",
       " ('ends', 115),\n",
       " ('surge', 115),\n",
       " ('insurance', 115),\n",
       " ('range', 115),\n",
       " ('brexit', 114),\n",
       " ('advertising', 114),\n",
       " ('both', 114),\n",
       " ('away', 114),\n",
       " ('digital', 114),\n",
       " ('medical', 113),\n",
       " ('claims', 113),\n",
       " ('mcdonalds', 113),\n",
       " ('vs', 113),\n",
       " ('june', 113),\n",
       " ('mcafee', 113),\n",
       " ('local', 112),\n",
       " ('trends', 112),\n",
       " ('steady', 112),\n",
       " ('ebay', 112),\n",
       " ('extraction', 112),\n",
       " ('nazi', 112),\n",
       " ('white', 111),\n",
       " ('april', 111),\n",
       " ('13', 111),\n",
       " ('equipment', 111),\n",
       " ('makes', 111),\n",
       " ('spy', 111),\n",
       " ('arrested', 111),\n",
       " ('usa', 111),\n",
       " ('interest', 110),\n",
       " ('party', 110),\n",
       " ('pepsico', 110),\n",
       " ('meeting', 110),\n",
       " ('fell', 110),\n",
       " ('usd', 110),\n",
       " ('leader', 109),\n",
       " ('cuts', 109),\n",
       " ('sea', 109),\n",
       " ('express', 109),\n",
       " ('another', 109),\n",
       " ('investigation', 109),\n",
       " ('roku', 109),\n",
       " ('model', 109),\n",
       " ('ago', 109),\n",
       " ('these', 109),\n",
       " ('much', 109),\n",
       " ('ab', 109),\n",
       " ('retail', 108),\n",
       " ('orders', 108),\n",
       " ('called', 108),\n",
       " ('googulxyz', 108),\n",
       " ('worlds', 108),\n",
       " ('private', 108),\n",
       " ('acquisition', 108),\n",
       " ('paper', 108),\n",
       " ('moderna', 108),\n",
       " ('based', 108),\n",
       " ('travelers', 108),\n",
       " ('december', 108),\n",
       " ('3m', 107),\n",
       " ('raises', 107),\n",
       " ('month', 107),\n",
       " ('experts', 107),\n",
       " ('release', 107),\n",
       " ('meat', 107),\n",
       " ('better', 107),\n",
       " ('opera', 106),\n",
       " ('sold', 106),\n",
       " ('outlook', 106),\n",
       " ('economic', 106),\n",
       " ('site', 105),\n",
       " ('uber', 105),\n",
       " ('players', 105),\n",
       " ('too', 105),\n",
       " ('again', 105),\n",
       " ('cement', 105),\n",
       " ('clean', 105),\n",
       " ('totestravel', 105),\n",
       " ('interface', 105),\n",
       " ('boris', 104),\n",
       " ('growing', 104),\n",
       " ('within', 104),\n",
       " ('qa', 104),\n",
       " ('without', 104),\n",
       " ('salesforce', 104),\n",
       " ('renewable', 103),\n",
       " ('cost', 103),\n",
       " ('area', 103),\n",
       " ('general', 103),\n",
       " ('official', 103),\n",
       " ('thousands', 103),\n",
       " ('negative', 103),\n",
       " ('boost', 103),\n",
       " ('lyft', 103),\n",
       " ('omx', 103),\n",
       " ('put', 102),\n",
       " ('won', 102),\n",
       " ('iphone', 102),\n",
       " ('program', 102),\n",
       " ('list', 102),\n",
       " ('korean', 102),\n",
       " ('slam', 102),\n",
       " ('dividend', 102),\n",
       " ('build', 101),\n",
       " ('california', 101),\n",
       " ('decision', 101),\n",
       " ('working', 101),\n",
       " ('least', 101),\n",
       " ('costs', 101),\n",
       " ('africa', 101),\n",
       " ('death', 101),\n",
       " ('sugar', 101),\n",
       " ('depot', 101),\n",
       " ('ammo', 101),\n",
       " ('root', 101),\n",
       " ('gopro', 101),\n",
       " ('black', 100),\n",
       " ('trial', 100),\n",
       " ('electronics', 100),\n",
       " ('holdings', 100),\n",
       " ('asian', 100),\n",
       " ('gan', 100),\n",
       " ('woodward', 100),\n",
       " ('sandeep', 100),\n",
       " ('caign', 99),\n",
       " ('size', 99),\n",
       " ('told', 99),\n",
       " ('solution', 99),\n",
       " ('dead', 99),\n",
       " ('lawsuit', 99),\n",
       " ('october', 99),\n",
       " ('team', 99),\n",
       " ('twitter', 99),\n",
       " ('authorities', 99),\n",
       " ('caterpillar', 99),\n",
       " ('hold', 99),\n",
       " ('members', 99),\n",
       " ('woman', 99),\n",
       " ('xp', 99),\n",
       " ('economy', 98),\n",
       " ('annual', 98),\n",
       " ('offers', 98),\n",
       " ('force', 98),\n",
       " ('staff', 98),\n",
       " ('22', 98),\n",
       " ('warning', 98),\n",
       " ('risk', 98),\n",
       " ('election', 98),\n",
       " ('dropbox', 98),\n",
       " ('great', 97),\n",
       " ('factory', 97),\n",
       " ('wants', 97),\n",
       " ('deals', 97),\n",
       " ('provide', 97),\n",
       " ('here', 97),\n",
       " ('la', 96),\n",
       " ('fears', 96),\n",
       " ('keep', 96),\n",
       " ('2016', 96),\n",
       " ('press', 96),\n",
       " ('legal', 96),\n",
       " ('industrial', 96),\n",
       " ('aapl', 96),\n",
       " ('large', 96),\n",
       " ('available', 95),\n",
       " ('buys', 95),\n",
       " ('airlines', 95),\n",
       " ('find', 95),\n",
       " ('sees', 95),\n",
       " ('windows', 95),\n",
       " ('concerns', 94),\n",
       " ('losses', 94),\n",
       " ('judge', 94),\n",
       " ('those', 94),\n",
       " ('23', 94),\n",
       " ('january', 94),\n",
       " ('111', 94),\n",
       " ('travel', 93),\n",
       " ('want', 93),\n",
       " ('fight', 93),\n",
       " ('hotels', 93),\n",
       " ('2011', 93),\n",
       " ('point', 93),\n",
       " ('decline', 93),\n",
       " ('n', 93),\n",
       " ('dips', 93),\n",
       " ('weeks', 92),\n",
       " ('history', 92),\n",
       " ('recovery', 92),\n",
       " ('goes', 92),\n",
       " ('education', 92),\n",
       " ('car', 92),\n",
       " ('reliance', 92),\n",
       " ('j', 92),\n",
       " ('intrusion', 92),\n",
       " ('nse', 92),\n",
       " ('19', 91),\n",
       " ('union', 91),\n",
       " ('euros', 91),\n",
       " ('america', 91),\n",
       " ('income', 91),\n",
       " ('networks', 91),\n",
       " ('looks', 91),\n",
       " ('ptc', 91),\n",
       " ('mohoni', 91),\n",
       " ('control', 90),\n",
       " ('sells', 90),\n",
       " ('looking', 90),\n",
       " ('brazil', 90),\n",
       " ('already', 90),\n",
       " ('significant', 89),\n",
       " ('potential', 89),\n",
       " ('others', 89),\n",
       " ('plants', 89),\n",
       " ('shareholders', 89),\n",
       " ('silver', 89),\n",
       " ('wagle', 89),\n",
       " ('block', 88),\n",
       " ('jet', 88),\n",
       " ('position', 88),\n",
       " ('analysts', 88),\n",
       " ('include', 88),\n",
       " ('murder', 88),\n",
       " ('govt', 87),\n",
       " ('launched', 87),\n",
       " ('charges', 87),\n",
       " ('corporate', 87),\n",
       " ('talks', 87),\n",
       " ('brands', 87),\n",
       " ('merck', 87),\n",
       " ('garmin', 87),\n",
       " ('wind', 87),\n",
       " ('ftse', 87),\n",
       " ('manufacturing', 86),\n",
       " ('behind', 86),\n",
       " ('estimates', 86),\n",
       " ('independent', 86),\n",
       " ('crude', 86),\n",
       " ('ukraine', 86),\n",
       " ('35', 86),\n",
       " ('issues', 86),\n",
       " ('run', 86),\n",
       " ('book', 86),\n",
       " ('bullish', 86),\n",
       " ('yuan', 86),\n",
       " ('flex', 86),\n",
       " ('oy', 86),\n",
       " ('infra', 86),\n",
       " ('huge', 85),\n",
       " ('possible', 85),\n",
       " ('intelligence', 85),\n",
       " ('warns', 85),\n",
       " ('secret', 85),\n",
       " ('signed', 85),\n",
       " ('six', 85),\n",
       " ('p', 85),\n",
       " ('choksey', 85),\n",
       " ('old', 84),\n",
       " ('store', 84),\n",
       " ('website', 84),\n",
       " ('hackers', 84),\n",
       " ('millions', 84),\n",
       " ('holding', 84),\n",
       " ('picks', 84),\n",
       " ('banned', 83),\n",
       " ('morgan', 83),\n",
       " ('bid', 83),\n",
       " ('must', 83),\n",
       " ('profits', 83),\n",
       " ('200', 83),\n",
       " ('hopes', 83),\n",
       " ('nine', 83),\n",
       " ('housing', 83),\n",
       " ('sites', 83),\n",
       " ('bet', 83),\n",
       " ('details', 82),\n",
       " ('reviews', 82),\n",
       " ('children', 82),\n",
       " ('syria', 82),\n",
       " ('max', 82),\n",
       " ('spot', 82),\n",
       " ('protest', 82),\n",
       " ('few', 82),\n",
       " ('known', 81),\n",
       " ('released', 81),\n",
       " ('vote', 81),\n",
       " ('pollution', 81),\n",
       " ('far', 81),\n",
       " ('political', 81),\n",
       " ('know', 81),\n",
       " ('full', 81),\n",
       " ('honeywell', 81),\n",
       " ('que', 81),\n",
       " ('stay', 81),\n",
       " ('region', 81),\n",
       " ('expects', 81),\n",
       " ('upside', 81),\n",
       " ('mehta', 81),\n",
       " ('60', 80),\n",
       " ('supreme', 80),\n",
       " ('soon', 80),\n",
       " ('resources', 80),\n",
       " ('traffic', 80),\n",
       " ('worst', 80),\n",
       " ('fine', 80),\n",
       " ('bets', 80),\n",
       " ('august', 80),\n",
       " ('nestle', 80),\n",
       " ('finds', 80),\n",
       " ('motors', 80),\n",
       " ('currency', 80),\n",
       " ('irish', 80),\n",
       " ('does', 80),\n",
       " ('longterm', 80),\n",
       " ('green', 79),\n",
       " ('marketing', 79),\n",
       " ('charged', 79),\n",
       " ('27', 79),\n",
       " ('paris', 79),\n",
       " ('outside', 79),\n",
       " ('each', 79),\n",
       " ('st', 79),\n",
       " ('engineering', 79),\n",
       " ('performance', 79),\n",
       " ('natural', 79),\n",
       " ('cancer', 78),\n",
       " ('paid', 78),\n",
       " ('poor', 78),\n",
       " ('signs', 78),\n",
       " ('palestinian', 78),\n",
       " ('star', 78),\n",
       " ('red', 78),\n",
       " ('telecom', 78),\n",
       " ('february', 78),\n",
       " ('mining', 77),\n",
       " ('holds', 77),\n",
       " ('coming', 77),\n",
       " ('administration', 77),\n",
       " ('york', 77),\n",
       " ('hike', 77),\n",
       " ('game', 77),\n",
       " ('worldwide', 77),\n",
       " ('tcs', 77),\n",
       " ('turn', 76),\n",
       " ('lost', 76),\n",
       " ('every', 76),\n",
       " ('urban', 76),\n",
       " ('held', 76),\n",
       " ('developed', 76),\n",
       " ('limited', 76),\n",
       " ...]"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = {}\n",
    "for column in df['Headline']:\n",
    "    for word in column.split(): \n",
    "        words[word] = words.get(word, 0) + 1\n",
    "\n",
    "sorted_words = sorted(words.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37478"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507767"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_sum = sum([b for a, b in sorted_words])\n",
    "words_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top50:  27.576427771005207\n",
      "top100:  33.009037609769834\n",
      "top250:  41.99938160613037\n",
      "top500:  50.86013860688071\n",
      "top1000:  61.078408009973074\n",
      "top2500:  74.65313815194764\n",
      "top5000:  83.78882440174333\n",
      "top10000:  91.06657187253208\n",
      "top25000:  97.54257366075385\n"
     ]
    }
   ],
   "source": [
    "nums = [50, 100, 250, 500, 1000, 2500, 5000, 10000, 25000]\n",
    "for num in nums:\n",
    "    ans = sum([b for a, b in sorted_words[:num]]) / words_sum * 100\n",
    "    print(f\"top{num}: \", ans)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can improve this by using lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kuba/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/kuba/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/kuba/nltk_data'\n    - '/home/kuba/miniconda3/envs/nlp/nltk_data'\n    - '/home/kuba/miniconda3/envs/nlp/share/nltk_data'\n    - '/home/kuba/miniconda3/envs/nlp/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - 'nltk_data'\n    - 'nltk_data'\n    - 'nltk_data'\n    - 'nltk_data'\n    - '/mnt/c/Users/jakub/programowanie/greatProjeckts/AI-trader/nlp/notebooks /nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[477], line 16\u001b[0m\n\u001b[1;32m      6\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# lemmatizer = WordNetLemmatizer()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# def lemmatize(text: str) -> str:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# lemmatize(\"i love reading the books\")\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI love reading the books\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/kuba/nltk_data'\n    - '/home/kuba/miniconda3/envs/nlp/nltk_data'\n    - '/home/kuba/miniconda3/envs/nlp/share/nltk_data'\n    - '/home/kuba/miniconda3/envs/nlp/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - 'nltk_data'\n    - 'nltk_data'\n    - 'nltk_data'\n    - 'nltk_data'\n    - '/mnt/c/Users/jakub/programowanie/greatProjeckts/AI-trader/nlp/notebooks /nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def lemmatize(text: str) -> str:\n",
    "#     words = word_tokenize(text)\n",
    "#     return ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
    "\n",
    "# lemmatize(\"i love reading the books\")\n",
    "\n",
    "word_tokenize(\"I love reading the books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it seems that it's enoug to use only 5000 most common words to tran the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/kuba/nltk_data'\n    - '/home/kuba/miniconda3/envs/nlp/nltk_data'\n    - '/home/kuba/miniconda3/envs/nlp/share/nltk_data'\n    - '/home/kuba/miniconda3/envs/nlp/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[441], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeadline\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHeadline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlemmatize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/pandas/core/series.py:4700\u001b[0m, in \u001b[0;36mSeries.map\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   4620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmap\u001b[39m(\n\u001b[1;32m   4621\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4622\u001b[0m     arg: Callable \u001b[38;5;241m|\u001b[39m Mapping \u001b[38;5;241m|\u001b[39m Series,\n\u001b[1;32m   4623\u001b[0m     na_action: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4624\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m   4625\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4626\u001b[0m \u001b[38;5;124;03m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[1;32m   4627\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4698\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[1;32m   4699\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4700\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_values, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[1;32m   4702\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4703\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[419], line 11\u001b[0m, in \u001b[0;36mlemmatize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlemmatize\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 11\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words])\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/kuba/nltk_data'\n    - '/home/kuba/miniconda3/envs/nlp/nltk_data'\n    - '/home/kuba/miniconda3/envs/nlp/share/nltk_data'\n    - '/home/kuba/miniconda3/envs/nlp/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "df['Headline'] = df['Headline'].map(lemmatize)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top50:  27.576427771005207\n",
      "top100:  33.009037609769834\n",
      "top250:  41.99938160613037\n",
      "top500:  50.86013860688071\n",
      "top1000:  61.078408009973074\n",
      "top2500:  74.65313815194764\n",
      "top5000:  83.78882440174333\n",
      "top10000:  91.06657187253208\n",
      "top25000:  97.54257366075385\n"
     ]
    }
   ],
   "source": [
    "words = {}\n",
    "for column in df['Headline']:\n",
    "    for word in column.split(): \n",
    "        words[word] = words.get(word, 0) + 1\n",
    "\n",
    "sorted_words = sorted(words.items(), key=lambda x: x[1], reverse=True)\n",
    "words_sum = sum([b for a, b in sorted_words])\n",
    "\n",
    "nums = [50, 100, 250, 500, 1000, 2500, 5000, 10000, 25000]\n",
    "for num in nums:\n",
    "    ans = sum([b for a, b in sorted_words[:num]]) / words_sum * 100\n",
    "    print(f\"top{num}: \", ans)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
